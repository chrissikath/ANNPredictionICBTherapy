{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T11:27:19.783882Z",
     "start_time": "2021-02-01T11:27:19.771105Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import csv\n",
    "import sklearn\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow.keras.backend as K\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from scipy import stats\n",
    "from sklearn import svm, datasets, metrics, model_selection, svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_curve, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RepeatedKFold, KFold, ShuffleSplit, StratifiedShuffleSplit\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet, LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T12:03:30.264742Z",
     "start_time": "2021-04-08T12:03:30.194698Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Funktionen\n",
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    Load csv file \n",
    "    Return: DataFrame \n",
    "    \"\"\"\n",
    "    print(\"-----Load Dataset-----\")\n",
    "    dataset = pd.read_csv(file)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def choose_input_columns(dataset, genes, clincial_variables, tpm_removal):\n",
    "    \"\"\"\n",
    "    Choose number of genes or specific genes [\"MET\",\"EYA1\"]\n",
    "    and clinical variables from dataset\n",
    "    Return: dataset_short (genes + clinical variables) \n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Choose input coloumns-----\")\n",
    "    if isinstance(genes, int):  # if genes == int: take all genes\n",
    "        print(str(genes) + \" genes taken.\")\n",
    "        genes = dataset.iloc[:, 1:genes]\n",
    "    elif str(genes) == '':  # if no genesfe\n",
    "        pass\n",
    "    else:  # take specific genes\n",
    "        print(str(genes) + \" taken.\")\n",
    "        genes = dataset[genes]\n",
    "\n",
    "    if tpm_removal == True:\n",
    "        print(\"-----Remove all Genes TPM <2-----\")\n",
    "        print(\"Number Genes: \", genes.shape[1])\n",
    "        obj = (genes < 2).all()\n",
    "        for key, value in obj.iteritems():\n",
    "            if value == True:\n",
    "                genes = genes.drop(columns=[key])\n",
    "        print(\"Number Genes after removed: \", genes.shape[1])\n",
    "    print()\n",
    "\n",
    "    dataset_variables = dataset[clincial_variables]\n",
    "    print(\"Clinical variables choosen: \")\n",
    "    print(dataset_variables.columns)\n",
    "    print()\n",
    "\n",
    "    if str(genes) == '':  # if no genes taken\n",
    "        dataset_short = dataset_variables\n",
    "    else:   # append genes with clinical variables\n",
    "        dataset_short = pd.concat([genes, dataset_variables], axis=1)\n",
    "\n",
    "    print(\"-----Remove NAs-----\")\n",
    "    print(\"With NAs: \", dataset_short.shape[0])\n",
    "    dataset_short = dataset_short.dropna()\n",
    "    print(\"Removed NA: \", dataset_short.shape[0])\n",
    "    print()\n",
    "    return dataset_short\n",
    "\n",
    "\n",
    "def choose_input_columns_with_genes_return(dataset, number_genes, clincial_variables):\n",
    "    \"\"\"\n",
    "    Same as choose_input_columns but also returns genes and dataset_variables seperatly\n",
    "    Use: merge gene set \n",
    "    Return: (dataset_short, genes, dataset_variables)\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Choose input coloumns-----\")\n",
    "    if isinstance(genes, int):  # if genes == int: take all genes\n",
    "        print(str(genes) + \" genes taken.\")\n",
    "        genes = dataset.iloc[:, 1:genes]\n",
    "    elif str(genes) == '':  # if no genes\n",
    "        pass\n",
    "    else:  # take specific genes\n",
    "        print(str(genes) + \" taken.\")\n",
    "        genes = dataset[genes]\n",
    "\n",
    "    dataset_variables = dataset[clincial_variables]\n",
    "    print(\"Clinical variables choosen: \")\n",
    "    print(dataset_variables.columns)\n",
    "\n",
    "    if str(genes) == '':  # if no genes taken\n",
    "        dataset_short = dataset_variables\n",
    "    else:   # append genes with clinical variables\n",
    "        dataset_short = pd.concat([genes, dataset_variables], axis=1)\n",
    "    return(dataset_short, genes, dataset_variables)\n",
    "\n",
    "\n",
    "def get_merge_genes(genes_Gide, genes_Liu):\n",
    "    \"\"\"\n",
    "    Merge two different gene set by names\n",
    "    Return: merged_genes \n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Get merged genes-----\")\n",
    "    merge_genes = []\n",
    "    for gene in genes_Gide.columns.tolist():\n",
    "        if gene in genes_Liu.columns.tolist():\n",
    "            merge_genes.append(gene)\n",
    "    print(\"Number merged genes: \", len(merge_genes))\n",
    "    return merge_genes\n",
    "\n",
    "\n",
    "def prepare_dataset(liu_or_gide, dataset, bin_categorical, string_to_binary, continous_to_binary, target):\n",
    "    \"\"\"\n",
    "    Prepare dataset \n",
    "    -> Features and Target structure \n",
    "    -> categorical values to binary (string_to_binary) gender (M,F) to 1,0 \n",
    "    -> continuous target to binary (continous_to_binary) OS to 1,0\n",
    "    -> binary string to binary value (bin_categorical) Tx to 1,0 \n",
    "    Return: (X,y)\n",
    "    \"\"\"\n",
    "    logging.info(\"------Prepare dataset: --------\")\n",
    "    logging.info(\"Bin_categorical: {}\".format(bin_categorical))\n",
    "    logging.info(\"string_to_binary: {}\".format(string_to_binary))\n",
    "    logging.info(\"continous_to_binary: {}\".format(continous_to_binary))\n",
    "    logging.info(\"Target: {}\".format(target))\n",
    "\n",
    "    # make OS:730 to 0 or 1\n",
    "    for key, value in continous_to_binary.items():\n",
    "        dataset[key] = np.where(\n",
    "            dataset[key] >= value, 1, 0)\n",
    "\n",
    "    # make gender M = 1 or F = 0\n",
    "    if string_to_binary != \"\":\n",
    "        value = string_to_binary\n",
    "        dataset[value] = np.where(dataset[value] == \"M\", 1, 0)\n",
    "\n",
    "    # make any other categorical value (Tx) to 1,0\n",
    "    if len(bin_categorical) != 0:\n",
    "        for elem in bin_categorical:\n",
    "            label = LabelEncoder()\n",
    "            dataset[elem] = label.fit_transform(dataset[elem])\n",
    "            \n",
    "    X = dataset.drop(labels=[target], axis=1)\n",
    "    y = dataset[target]\n",
    "    logging.info(X.head())\n",
    "    logging.info(y.head())\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "def grid_search_and_val(directory, X_train, y_train, X_test, y_test, model_type, param_grid, save_model, batch_size,\n",
    "                        earlystopping_patience, n_features, epochs=500):\n",
    "    \"\"\"\n",
    "    Grid SearchCV with train dataset and Validation with test datatset\n",
    "    Print Best Params\n",
    "    Print Confusion Matrix Results \n",
    "\n",
    "    Return: np.mean(scores), np.mean(sens_list), np.mean(spec_list), np.mean(ppv_list), np.mean(npv_list)\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----GridSearch------\")\n",
    "    logging.info(\"-----GridSearch------\")\n",
    "    logging.info(\"Used param grid: {}\".format(param_grid))\n",
    "\n",
    "    f = open(directory+\"/\"+\"param_grid.txt\", \"a+\")\n",
    "    for key in param_grid:\n",
    "        f.write(key+\":\"+str(param_grid[key])+\"\\n\")\n",
    "\n",
    "    f = open(directory+\"/\"+\"used_params.txt\", \"w+\")\n",
    "    f.write(\"epochs: \" + str(epochs) + \"\\n\" + \"batch size: \" + str(batch_size) + \"\\n\" +\n",
    "            \"earlystopping: \" + str(earlystopping_patience) + \"\\n\" + \"N_features: \" +\n",
    "            str(n_features) + \" (Only with Feature Selection)\")\n",
    "    f.close()\n",
    "\n",
    "    # Early stopping criteria\n",
    "#     early_stopping_cb = kecras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0, patience=earlystopping_patience, mode='auto')\n",
    "\n",
    "    learning_rate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "    callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "    # Three different types of ANNs (binary, continuously, autoencoder)\n",
    "    if model_type == \"binary\":\n",
    "        print(\"Binary Model\")\n",
    "        logging.info(\"Binary Model\")\n",
    "        ann = KerasClassifier(build_fn=binaryModel, input_shape=X_train.shape[1],\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_split=0.2)\n",
    "    elif model_type == \"continuously\":\n",
    "        print(\"Continuously Model\")\n",
    "        logging.info(\"Continuously Model\")\n",
    "        ann = KerasRegressor(build_fn=continuouslyModel, input_shape=X_train.shape[1],\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_split=0.2)\n",
    "    elif model_type == \"autoencoder\":\n",
    "        print(\"Autoencoder Model\")\n",
    "        logging.info(\"Autoencoder Model\")\n",
    "        ann = KerasRegressor(build_fn=binaryModelAutoencoder, input_shape=X_train.shape[1],\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_split=0.2)\n",
    "\n",
    "    # Pipeline for GridSearchCV (first StandardScaler then apply ann)\n",
    "    pipe = Pipeline(steps=[('scaler', StandardScaler()), ('ann', ann)])\n",
    "    grid_search_result = GridSearchCV(\n",
    "        pipe, param_grid=param_grid, n_jobs=1, cv=5)\n",
    "    grid_search_result = grid_search_result.fit(X_train, y_train,\n",
    "                                                ann__callbacks=callbacks, ann__verbose=0)\n",
    "\n",
    "    print()\n",
    "    print(\"-----Grid Search Result-----\")\n",
    "    logging.info(\"-----Grid Search Result-----\")\n",
    "    logging.info(\"Best: %f using %s\" %\n",
    "                 (grid_search_result.best_score_, grid_search_result.best_params_))\n",
    "    print(\"Best: %f using %s\" %\n",
    "          (grid_search_result.best_score_, grid_search_result.best_params_))\n",
    "\n",
    "    f = open(directory+\"/\"+\"best_params.txt\", \"a+\")\n",
    "    for key in grid_search_result.best_params_:\n",
    "        f.write(key+\":\"+str(grid_search_result.best_params_[key])+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    print()\n",
    "    print(\"-----Validation with test set-----\")\n",
    "    logging.info(\"-----Validation with test set-----\")\n",
    "    # Scale train data and transform train data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    if model_type == \"continuously\":\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        rmse_scores = []\n",
    "        for i in range(30):\n",
    "            print(\"Validation-Run: \", i)\n",
    "            model = continuouslyModel(\n",
    "                input_shape=X_train.shape[1],\n",
    "                n_neurons=grid_search_result.best_params_['ann__n_neurons'],\n",
    "                learning_rate=grid_search_result.best_params_[\n",
    "                    'ann__learning_rate'],\n",
    "                used_optimizer =  grid_search_result.best_params_['ann__used_optimizer'],\n",
    "                l1_reg=grid_search_result.best_params_['ann__l1_reg'],\n",
    "                l2_reg=grid_search_result.best_params_['ann__l2_reg'],\n",
    "                num_hidden=grid_search_result.best_params_['ann__num_hidden'],\n",
    "                dropout_rate=grid_search_result.best_params_['ann__dropout_rate'])\n",
    "\n",
    "            history = model.fit(X_train, y_train, callbacks=callbacks, shuffle=True,\n",
    "                                batch_size=batch_size, validation_split=0.2, epochs=epochs, verbose=0)\n",
    "\n",
    "            test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "\n",
    "            if not os.path.exists(directory+\"/iteration_\"+str(i)):\n",
    "                os.makedirs(directory+\"/iteration_\"+str(i))\n",
    "\n",
    "            plt.clf()\n",
    "            for j in range(0, int(len(history.history.keys())/2)):\n",
    "                plt.plot(history.history[list(history.history.keys())[j]])\n",
    "                plt.plot(history.history['val_' +\n",
    "                                         list(history.history.keys())[j]])\n",
    "                plt.title('model ' + list(history.history.keys())[j])\n",
    "                plt.ylabel(list(history.history.keys())[j])\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "                plt.savefig(directory+'/iteration_'+str(i)+\"/\" +\n",
    "                            str(list(history.history)[j])+'.png')\n",
    "                plt.clf()\n",
    "\n",
    "            # predict\n",
    "            y_predicted = model.predict(X_test)\n",
    "\n",
    "            count = 0\n",
    "            for i in y_test:\n",
    "                print(i, y_predicted[count])\n",
    "                count += 1\n",
    "\n",
    "            rmse = (np.sqrt(mean_squared_error(y_test, y_predicted)))\n",
    "            r_squared = r2_score(y_test, y_predicted)\n",
    "\n",
    "            # append in lists\n",
    "            mae_scores.append(test_mae)\n",
    "            rmse_scores.append(rmse)\n",
    "            r2_scores.append(r_squared)\n",
    "\n",
    "        if save_model == True:\n",
    "            model_json = model.to_json()\n",
    "            with open(directory+\"/\"+\"model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            ann_val.save_weights(directory+\"/\"+\"model.h5\")\n",
    "            loggin.info(\"Saved model to disk\")\n",
    "\n",
    "        f = open(directory+\"/\"+\"metrics.txt\", \"a+\")\n",
    "        f.write(\n",
    "            \"MAE {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(mae_scores),  np.std(mae_scores)))\n",
    "        f.write(\n",
    "            \"RMSE {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(rmse_scores),  np.std(rmse_scores)))\n",
    "        f.write(\n",
    "            \"R^2 {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(r2_scores),  np.std(r2_scores)))\n",
    "        f.close()\n",
    "\n",
    "        print(\n",
    "            \"MAE {0:.2f} +/-({1:.2f}) \".format(np.mean(mae_scores),  np.std(mae_scores)))\n",
    "        print(\n",
    "            \"RMSE {0:.2f} +/-({1:.2f}) \".format(np.mean(rmse_scores),  np.std(rmse_scores)))\n",
    "        print(\n",
    "            \"R^2 {0:.2f} +/-({1:.2f}) \".format(np.mean(r2_scores),  np.std(r2_scores)))\n",
    "    else:\n",
    "        scores = []\n",
    "        sens_list = []\n",
    "        spec_list = []\n",
    "        ppv_list = []\n",
    "        npv_list = []\n",
    "        for i in range(30):  # Validation with test dataset (30 times)\n",
    "            print(\"Validation-Run: \", i)\n",
    "            # define best params from GridSearch\n",
    "            if model_type == \"binary\":\n",
    "                ann_val = binaryModel(input_shape=X_train.shape[1], \n",
    "                                      n_neurons=grid_search_result.best_params_['ann__n_neurons'],\n",
    "                                      learning_rate=grid_search_result.best_params_['ann__learning_rate'],\n",
    "                                     used_optimizer =  grid_search_result.best_params_['ann__used_optimizer'],\n",
    "                                      l1_reg=grid_search_result.best_params_['ann__l1_reg'],\n",
    "                                    l2_reg=grid_search_result.best_params_['ann__l2_reg'],\n",
    "                                    num_hidden=grid_search_result.best_params_['ann__num_hidden'],\n",
    "                                    dropout_rate=grid_search_result.best_params_['ann__dropout_rate'])\n",
    "                \n",
    "                #(input_shape, n_neurons, num_hidden, learning_rate, optimizer, l1_reg, l2_reg, dropout_rate):\n",
    "            elif model_type == \"autoencoder\":\n",
    "                ann_val = binaryModelAutoencoder(input_shape=X_train.shape[1], n_neurons=grid_search_result.best_params_['ann__n_neurons'],\n",
    "                                                 learning_rate=grid_search_result.best_params_[\n",
    "                    'ann__learning_rate'],optimizer = grid_search_result.best_parms_['ann__optimizer'],\n",
    "                    l1_reg=grid_search_result.best_params_[\n",
    "                    'ann__l1_reg'],\n",
    "                    l2_reg=grid_search_result.best_params_[\n",
    "                    'ann__l2_reg'],\n",
    "                    num_hidden=grid_search_result.best_params_[\n",
    "                        'ann__num_hidden'],\n",
    "                    dropout_rate=grid_search_result.best_params_['ann__dropout_rate'])\n",
    "\n",
    "            # fit ann with best params\n",
    "            history = ann_val.fit(X_train, y_train, batch_size=batch_size, verbose=0,\n",
    "                                  validation_split=0.2, shuffle=True, callbacks=callbacks, epochs=epochs)\n",
    "\n",
    "            if not os.path.exists(directory+\"/iteration_\"+str(i)):\n",
    "                os.makedirs(directory+\"/iteration_\"+str(i))\n",
    "\n",
    "            # plot training history\n",
    "            plt.clf()\n",
    "            for j in range(0, int(len(history.history.keys())/2)):\n",
    "                plt.plot(history.history[list(history.history.keys())[j]])\n",
    "                plt.plot(history.history['val_' +\n",
    "                                         list(history.history.keys())[j]])\n",
    "                plt.title('model ' + list(history.history.keys())[j])\n",
    "                plt.ylabel(list(history.history.keys())[j])\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train', 'test'], loc='upper left')\n",
    "                plt.show()\n",
    "                plt.savefig(directory+'/iteration_'+str(i)+\"/\" +\n",
    "                            str(list(history.history)[j])+'.png')\n",
    "                plt.close()\n",
    "\n",
    "            # evaluate with test set\n",
    "            test_loss, test_acc = ann_val.evaluate(X_test, y_test, verbose=0)\n",
    "            y_predicted = (ann_val.predict(X_test) > 0.5).astype(\"int32\")\n",
    "            print(y_predicted)\n",
    "            count = 0\n",
    "            for test_object in y_test:\n",
    "                print(test_object, y_predicted[count])\n",
    "                count += 1\n",
    "\n",
    "            sens, spec, ppv, npv = sens_spec_ppn_npv(\n",
    "                y_test, y_predicted)  # calculate confusion matrix\n",
    "            scores.append(test_acc)\n",
    "            sens_list.append(sens)\n",
    "            spec_list.append(spec)\n",
    "            ppv_list.append(ppv)\n",
    "            npv_list.append(npv)\n",
    "\n",
    "            # plot confusion matrix of last validation run\n",
    "            fig = plt.figure(figsize=(4, 4))\n",
    "            sns.heatmap(confusion_matrix(y_test, y_predicted, labels=[0,1]),\n",
    "                        vmax=.8, square=True, annot=True)\n",
    "            plt.show()\n",
    "            plt.savefig(directory+'/iteration_'+str(i) +\n",
    "                        \"/\"+'confusion_matrix.png')\n",
    "            plt.close()\n",
    "\n",
    "        plt.clf()\n",
    "        # plot confusion matrix of last validation run\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        sns.heatmap(confusion_matrix(y_test, y_predicted, labels=[0,1]),\n",
    "                    vmax=.8, square=True, annot=True)\n",
    "        plt.show()\n",
    "        plt.savefig(directory+\"/\"+'last_run_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        if save_model == True:\n",
    "            model_json = ann_val.to_json()\n",
    "            with open(directory+\"/\"+\"model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            ann_val.save_weights(directory+\"/\"+\"model.h5\")\n",
    "            logging.info(\"Saved model to disk\")\n",
    "\n",
    "        f = open(directory+\"/\"+\"metrics.txt\", \"a+\")\n",
    "        f.write(\n",
    "            \"Accuracy {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(scores),  np.std(scores)))\n",
    "        f.write(\n",
    "            \"Sensitivity {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(sens_list),  np.std(sens_list)))\n",
    "        f.write(\n",
    "            \"Specificity {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(spec_list),  np.std(spec_list)))\n",
    "        f.write(\n",
    "            \"PPV {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(ppv_list),  np.std(ppv_list)))\n",
    "        f.write(\n",
    "            \"NPV {0:.2f} +/-({1:.2f}) \\n\".format(np.mean(npv_list),  np.std(npv_list)))\n",
    "        f.close()\n",
    "\n",
    "        # print Results\n",
    "        print(\n",
    "            \"Accuracy {0:.2f} +/-({1:.2f}) \".format(np.mean(scores),  np.std(scores)))\n",
    "        print(\n",
    "            \"Sensitivity {0:.2f} +/-({1:.2f}) \".format(np.mean(sens_list),  np.std(sens_list)))\n",
    "        print(\n",
    "            \"Specificity {0:.2f} +/-({1:.2f}) \".format(np.mean(spec_list),  np.std(spec_list)))\n",
    "        print(\n",
    "            \"PPV {0:.2f} +/-({1:.2f}) \".format(np.mean(ppv_list),  np.std(ppv_list)))\n",
    "        print(\n",
    "            \"NPV {0:.2f} +/-({1:.2f}) \".format(np.mean(npv_list),  np.std(npv_list)))\n",
    "\n",
    "    logging.info(\"-----ANN and Validation Done-----\")\n",
    "\n",
    "\n",
    "def binaryModel(input_shape, n_neurons, num_hidden, learning_rate, used_optimizer, l1_reg, l2_reg, dropout_rate):\n",
    "    '''\n",
    "    Binary classification ANN Model with dropout layer \n",
    "    Optimize: \n",
    "    -> number neurons (n_neurons)\n",
    "    -> learning rate adam (learning_rate)\n",
    "    -> dropout rate (dropout_rate)\n",
    "\n",
    "    Return: Model \n",
    "    '''\n",
    "    inputs = Input((input_shape,))\n",
    "    for i in range(num_hidden):\n",
    "        hidden = Dense(n_neurons, activation=\"elu\",\n",
    "                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_reg, l2_reg))(inputs)\n",
    "        dropout = Dropout(dropout_rate)(hidden)\n",
    "    \n",
    "    outputs = (Dense(1, activation=\"sigmoid\"))(dropout)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    if used_optimizer == \"sgd\":\n",
    "        ann_optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate, momentum=0.5, nesterov=True)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=ann_optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    elif used_optimizer == \"adam\":\n",
    "        ann_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=ann_optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "    # plot graph\n",
    "    plot_model(model, to_file='multilayer_perceptron_graph_binary.png')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def binaryModelAutoencoder(input_shape, n_neurons, num_hidden, learning_rate, regularization_rate, dropout_rate):\n",
    "    '''\n",
    "    Binary classification ANN Model with dropout layer -> Autoencoder\n",
    "    Optimize: \n",
    "    -> number neurons (n_neurons)\n",
    "    -> learning rate adam (learning_rate)\n",
    "    -> dropout rate (dropout_rate)\n",
    "\n",
    "    Return: Model \n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_shape, activation=\"elu\",\n",
    "                    input_dim=input_shape))  # First Layer\n",
    "\n",
    "    model.add(Dense(265, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "\n",
    "#     for i in range(num_hidden):  # number hidden layers\n",
    "#         model.add(Dense(n_neurons, activation=\"relu\",\n",
    "#                         kernel_regularizer=tf.keras.regularizers.l1(regularization_rate)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    #optimizer = keras.optimizers.SGD(learning_rate=learning_rate , momentum =0.5)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def continuouslyModel(input_shape, n_neurons, num_hidden, learning_rate, used_optimizer, l1_reg, l2_reg, dropout_rate):\n",
    "    '''\n",
    "    Regression ANN Model with l1 regularization\n",
    "    Optimize: \n",
    "    -> number neurons (n_neurons)\n",
    "    -> learning rate adam (learning_rate)\n",
    "    -> regularizaion rate l1 (regularization_rate)\n",
    "\n",
    "    Metrics: accuracy, rmse, r2_keras\n",
    "\n",
    "    Return: Model \n",
    "    '''\n",
    "    inputs = Input((input_shape,))\n",
    "    for i in range(num_hidden):\n",
    "        hidden = Dense(n_neurons, activation=\"elu\",\n",
    "                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_reg, l2_reg))(inputs)\n",
    "        dropout = Dropout(dropout_rate)(hidden)\n",
    "    \n",
    "    outputs = (Dense(1, activation=\"linear\"))(dropout)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    if used_optimizer == \"sgd\":\n",
    "        ann_optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate, momentum=0.5, nesterov=True, clipnorm=1.)\n",
    "        model.compile(loss=\"mse\", optimizer=ann_optimizer,\n",
    "                  metrics=['mae'])\n",
    "        \n",
    "    elif used_optimizer == \"adam\":\n",
    "        ann_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.)\n",
    "        model.compile(loss=\"mse\", optimizer=ann_optimizer,\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "    # plot graph\n",
    "    plot_model(model, to_file='multilayer_perceptron_graph_continuously.png')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def feature_Selection(X, y, n_features=150, step=100, verbose=1):\n",
    "    \"\"\"\n",
    "    Feature Selection withs SVM linear kernel (step = 100)\n",
    "    Variabel Step, verbose and n_features \n",
    "\n",
    "    Input: \n",
    "    -> X = features\n",
    "    -> y = target\n",
    "    -> n_features = Number features to choose \n",
    "\n",
    "    Return: (selector, selected X, columns, ranks)\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Feature Selection-----\")\n",
    "    estimator = SVC(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=n_features,\n",
    "                   step=step, verbose=verbose)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    columns = []\n",
    "    ranks = {}\n",
    "    for i in range(X.shape[1]):\n",
    "        if selector.support_[i] == True:\n",
    "            columns.append(i)\n",
    "        d = {str(selector.ranking_[i]): str(i)}\n",
    "        ranks.update(d)\n",
    "        #print('Column: %d, Selected %s, Rank: %.3f' % (i, selector.support_[i], selector.ranking_[i]))\n",
    "\n",
    "    print(\"-----Feature Selection DONE------\")\n",
    "    return (selector, X_selected, columns, ranks)\n",
    "\n",
    "\n",
    "def featureSelection_selector(X, y, n_features=150):\n",
    "    \"\"\"\n",
    "    Feature Selection withs SVM linear kernel (step = 100)\n",
    "    Use: train on Liu, transform on Gide \n",
    "\n",
    "    Input: \n",
    "    -> X = features\n",
    "    -> y = target\n",
    "    -> n_features = Number features to choose \n",
    "\n",
    "    Return: selector \n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Feature Selection-----\")\n",
    "    estimator = SVC(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=n_features,\n",
    "                   step=100, verbose=1)\n",
    "    X = selector.fit_transform(X, y)\n",
    "    return selector\n",
    "\n",
    "\n",
    "def crossValidationContinuously(X_train, y_train, X_test, y_test, best_learning_rate, best_n_neurons, epochs=300, batch_size=32):\n",
    "    \"\"\"\n",
    "    Cross Validation for Continuously Model\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"-----Cross Validation-----\")\n",
    "    scores = []\n",
    "    msescores = []\n",
    "    rsscores = []\n",
    "    rs_score = []\n",
    "    for i in range(30):\n",
    "        print(\"Validation-Run: \", i)\n",
    "        model = continuouslyModel(\n",
    "            input_shape=X_train.shape[1], n_neurons=best_n_neurons, learning_rate=best_learning_rate)\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=50)\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, callbacks=[\n",
    "                            early_stopping_cb], validation_split=0.2, epochs=epochs, verbose=0)\n",
    "        test_loss, test_rmse, test_rsquared = model.evaluate(X_test, y_test)\n",
    "        # predict\n",
    "        y_predicted = model.predict_classes(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_predicted)\n",
    "        r_squared = r2_score(y_test, y_predicted)\n",
    "        # append in lists\n",
    "        scores.append(test_rmse)\n",
    "        msescores.append(mae)\n",
    "        rsscores.append(r_squared)\n",
    "        rs_score.append(test_rsquared)\n",
    "\n",
    "    plotHistory(history)\n",
    "    # Beispiel des letzten Durchlaufs\n",
    "    #fig = plt.figure(figsize = (4,4))\n",
    "    #sns.heatmap(confusion_matrix(y_test, y_predicted), vmax = .8, square = True, annot=True)\n",
    "    # plt.show()\n",
    "\n",
    "    print(\"RMSE %.2f%% (+/- %.2f%%)\" % (np.mean(scores), np.std(scores)))\n",
    "    print(\"MAE %.2f%% (+/- %.2f%%)\" % (np.mean(msescores), np.std(msescores)))\n",
    "    print(\"R Squared %.2f%% (+/- %.2f%%)\" %\n",
    "          (np.mean(rsscores), np.std(rsscores)))\n",
    "    print(\"R Squared metric %.2f%% (+/- %.2f%%)\" %\n",
    "          (np.mean(rs_score), np.std(rs_score)))\n",
    "\n",
    "\n",
    "def sens_spec_ppn_npv(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate sensitivity, specificity, PPV, NPV\n",
    "\n",
    "    Params: \n",
    "    y_pred - Predicted labels\n",
    "    y_true - True labels \n",
    "\n",
    "    Returns:  (sensitivity, specificity, PPV, NPV)\n",
    "    \"\"\"\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    PPV = TP/(TP+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    return (sensitivity, specificity, PPV, NPV)\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true)*(1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "\n",
    "def clean_out_Gide(dataset_Gide):\n",
    "    \"\"\"\n",
    "    Clean out Gide dataset \n",
    "    -> drop NAs\n",
    "    -> only use PD1\n",
    "    -> only use PRE\n",
    "    -> only use non BRAF mutation\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"----Clean Out Datasets-----\")\n",
    "    print(\"Gide\")\n",
    "    # Prepare Gide\n",
    "    indexNames = dataset_Gide[dataset_Gide['Treatment_'] != \"PD1\"].index\n",
    "    # Delete rowse where Treatment is not PD1\n",
    "    dataset_Gide.drop(indexNames, inplace=True)\n",
    "    print(len(dataset_Gide), \" nur PD1\")\n",
    "\n",
    "    indexNames = dataset_Gide[dataset_Gide['RNAseq'] != \"PRE\"].index\n",
    "    # Delete rowse where RNASeq is not PRE\n",
    "    dataset_Gide.drop(indexNames, inplace=True)\n",
    "    print(len(dataset_Gide), \" nur PRE\")\n",
    "\n",
    "    indexNames = dataset_Gide[dataset_Gide['BRAF_V600_mutation'] != 0].index\n",
    "    dataset_Gide.drop(indexNames, inplace=True)\n",
    "    print(len(dataset_Gide), \" ohne BRAF\")\n",
    "\n",
    "    return dataset_Gide\n",
    "\n",
    "\n",
    "def clean_out_Liu(dataset_Liu):\n",
    "    \"\"\"\n",
    "    Clean out Liu dataset \n",
    "    -> drop NAs\n",
    "    -> only use numPrior == 0\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"----Clean Out Dataset-----\")\n",
    "    # Prepare Liu\n",
    "    print(\"Liu\")\n",
    "\n",
    "    #indexNames = dataset_Liu[ dataset_Liu['numPriorTherapies'] !=  0].index\n",
    "    #dataset_Liu.drop(indexNames , inplace=True)\n",
    "    #print(len(dataset_Liu), \"ohne post Therapien\")\n",
    "    return dataset_Liu\n",
    "\n",
    "\n",
    "def clean_out_Riaz(dataset_Riaz):\n",
    "    \"\"\"\n",
    "    Clean out Riaz dataset \n",
    "    -> drop NAs\n",
    "    -> only use PRE\n",
    "    -> change os_week to os_days \n",
    "    \"\"\"\n",
    "    print(\"Riaz\")\n",
    "    indexNames = dataset_Riaz[dataset_Riaz['On_Pre'] != \"Pre\"].index\n",
    "    dataset_Riaz.drop(indexNames, inplace=True)\n",
    "    print(len(dataset_Riaz), \"ohne On_Pre\")\n",
    "\n",
    "    print(\"change weeks to days\")\n",
    "    # os_weeks in days umrechnen\n",
    "    dataset_Riaz['os_weeks'] = dataset_Riaz['os_weeks']*7\n",
    "\n",
    "    return dataset_Riaz\n",
    "\n",
    "\n",
    "def overview_data(dataset, clinical):\n",
    "    \"\"\"\n",
    "    Get overview of data\n",
    "    -> Summary \n",
    "    -> plot clinical values \n",
    "    -> plot correlations of clinical values \n",
    "    \"\"\"\n",
    "    print(dataset.describe())\n",
    "    shortDataset = dataset[clinical]\n",
    "\n",
    "    shortDataset.hist(figsize=(12, 10))\n",
    "    plt.show()\n",
    "\n",
    "    C_mat = shortDataset.corr()\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    sns.heatmap(C_mat, vmax=.8, square=True, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotHistory(history):\n",
    "    \"\"\"\n",
    "    Plot training history of ANN\n",
    "    \"\"\"\n",
    "    for i in range(0, int(len(history.history.keys())/2)):\n",
    "        plt.plot(history.history[list(history.history.keys())[i]])\n",
    "        plt.plot(history.history['val_' + list(history.history.keys())[i]])\n",
    "        plt.title('model ' + list(history.history.keys())[i])\n",
    "        plt.ylabel(list(history.history.keys())[i])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    RMSE metric function\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    r2 metric function \n",
    "    \"\"\"\n",
    "    SS_res = K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "\n",
    "def drop_outliers(dataset, features):\n",
    "    \"\"\"\n",
    "    Drop Outliers for features where z value > 3\n",
    "    Return dataset without outliers \n",
    "    \"\"\"\n",
    "#     z = np.abs(stats.zscore(dataset))\n",
    "#     outliers = (np.where(z > 3))\n",
    "#     print(outliers )\n",
    "\n",
    "    print(\"------Delete outliers------\")\n",
    "    for feature in features:\n",
    "        print(feature)\n",
    "        # z score for this feature\n",
    "        z_scores = np.abs(stats.zscore(dataset[feature]))\n",
    "\n",
    "        outliers = (np.where(z_scores > 3, True, False))\n",
    "        print(np.where(outliers == True)[0])\n",
    "        for i in (np.where(outliers == True)[0]):\n",
    "            try:\n",
    "                dataset = dataset.drop(i)\n",
    "                dataset = dataset.reset_index(drop=True)\n",
    "            except:\n",
    "                print(\"Row already dropped.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def feature_selection_logreg(param_grid, X, X_train, y_train, X_test, y_test):\n",
    "    print()\n",
    "    print(\"------Feature Selection via LogReg ElasticNet-------\")\n",
    "    print(\"------LogReg ElasticNet Tuning-------\")\n",
    "    # LogisticRegression Model with elastic net\n",
    "    log_reg = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', max_iter=5000, verbose=2, n_jobs=-1)\n",
    "\n",
    "    grid_search_result = GridSearchCV(\n",
    "        log_reg, param_grid=param_grid, n_jobs=1, cv=2)\n",
    "    grid_search_result = grid_search_result.fit(\n",
    "        X_train, y_train)\n",
    "\n",
    "    print(\"-----Grid Search Result-----\")\n",
    "    print(\"Best: %f using %s\" %\n",
    "          (grid_search_result.best_score_, grid_search_result.best_params_))\n",
    "\n",
    "    print(\"------Feature Selection-------\")\n",
    "    # LogisticRegression with best params\n",
    "    log_reg_best_param = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', max_iter=5000, verbose=2, n_jobs=-1,\n",
    "        C=grid_search_result.best_params_['C'],\n",
    "        l1_ratio=grid_search_result.best_params_['l1_ratio'])\n",
    "\n",
    "    log_reg_best_param.fit(X_train, y_train)\n",
    "    y_predicted = log_reg_best_param.predict(X_test)\n",
    "    print(\"Score Test Dataset:\")\n",
    "    print(log_reg_best_param.score(X_test, y_test))\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_predicted),\n",
    "                vmax=.8, square=True, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return short X_train, X_test based on selected features\n",
    "    selected_coeff = (\n",
    "        np.abs(np.round(log_reg_best_param.coef_, decimals=2)) > 0)[0]\n",
    "\n",
    "    print(\"Länge selected coeff\", len(selected_coeff))\n",
    "    print(\"Länge X_train\", X_train.shape)\n",
    "    print(type(selected_coeff))\n",
    "\n",
    "    # select from dataframe by boolean list\n",
    "    X_selected_train = X_train[:, selected_coeff]\n",
    "    X_selected_test = X_test[:, selected_coeff]\n",
    "\n",
    "    print(\"Number selected coefficients\", len(X_selected_train[0]))\n",
    "\n",
    "    odds = np.exp(log_reg_best_param.coef_[0])\n",
    "    not_odds = (log_reg_best_param.coef_[0])\n",
    "\n",
    "    coefficients = pd.DataFrame(not_odds,\n",
    "                                X.columns,\n",
    "                                columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "\n",
    "    coefficient_names = X.columns[selected_coeff]\n",
    "    print(\"Features selected \", coefficient_names)\n",
    "    coefficients_with_value = coefficients.loc[coefficient_names, ]\n",
    "\n",
    "    return (coefficients_with_value, X_selected_train, X_selected_test)\n",
    "\n",
    "\n",
    "def feature_selection_svc(param_grid, binary, X, X_train, y_train, X_test, y_test, n_features, step=100, verbose=1):\n",
    "    print()\n",
    "    print(\"------Feature Selection via SVC-------\")\n",
    "    logging.info(\"------Feature Selection via SVC-------\")\n",
    "    logging.info(\"------SVC Fine Tuning-------\")\n",
    "    if binary == True:\n",
    "        clf = svm.SVC()\n",
    "    else:\n",
    "        clf = svm.SVR()\n",
    "\n",
    "    grid_search_result = GridSearchCV(\n",
    "        clf, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "    grid_search_result = grid_search_result.fit(\n",
    "        X_train, y_train)\n",
    "\n",
    "    logging.info(\"-----Grid Search Result-----\")\n",
    "    logging.info(\"Best: %f using %s\" %\n",
    "                 (grid_search_result.best_score_, grid_search_result.best_params_))\n",
    "\n",
    "    # LogisticRegression with best params\n",
    "    if binary == True:\n",
    "        clf_bestparams = svm.SVC(\n",
    "            kernel=grid_search_result.best_params_['kernel'],\n",
    "            C=grid_search_result.best_params_['C'])\n",
    "    else:\n",
    "        clf_bestparams = svm.SVR(\n",
    "            kernel=grid_search_result.best_params_['kernel'],\n",
    "            C=grid_search_result.best_params_['C'])\n",
    "\n",
    "    clf_bestparams.fit(X_train, y_train)\n",
    "    y_predicted = clf_bestparams.predict(X_test)\n",
    "    logging.info(\"Score Test Dataset: {}\".format(\n",
    "        clf_bestparams.score(X_test, y_test)))\n",
    "    if binary == True:\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        sns.heatmap(confusion_matrix(y_test, y_predicted),\n",
    "                    vmax=.8, square=True, annot=True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        rmse = (np.sqrt(mean_squared_error(y_test, y_predicted)))\n",
    "        r_squared = r2_score(y_test, y_predicted)\n",
    "        logging.info(\"RMSE: {}\".format(rmse))\n",
    "        logging.info(\"R Squared: {}\".format(r_squared))\n",
    "\n",
    "    logging.info(\"------Feature Selection------\")\n",
    "    if binary == True:\n",
    "        estimator = SVC(kernel=grid_search_result.best_params_[\n",
    "                        'kernel'], C=grid_search_result.best_params_['C'])\n",
    "    else:\n",
    "        estimator = SVR(kernel=grid_search_result.best_params_[\n",
    "                        'kernel'], C=grid_search_result.best_params_['C'])\n",
    "\n",
    "    selector = RFE(estimator, n_features_to_select=n_features,\n",
    "                   step=step, verbose=verbose)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    columns = []\n",
    "    logging.info(selector.support_)\n",
    "    for i in range(X_train.shape[1]):\n",
    "        if selector.support_[i] == True:\n",
    "            columns.append(i)\n",
    "\n",
    "    logging.info(\"-----Feature Selection DONE------\")\n",
    "    coefficients = X.columns[columns]\n",
    "\n",
    "    return (coefficients, selector, X_train_selected, X_test_selected)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    learning_rate = initial_lrate * math.pow(drop,\n",
    "                                             math.floor((1+epoch)/epochs_drop))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def load_model_and_predict(model_json, model_h5, binary, table):\n",
    "    \"\"\"\n",
    "    Takes the model in json and h5 format and predicts the outcome for given table \n",
    "    Params:\n",
    "    model_json - model in json format\n",
    "    model_h5 - model in h5 format\n",
    "    binary - True for binary outcome, False for continuously outcome \n",
    "    table - Predict outcome for given table\n",
    "    \"\"\"\n",
    "    # load json and create model\n",
    "    data = load_data(table)\n",
    "    json_file = open(model_json, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_h5)\n",
    "    print(\"Loaded model from disk\")\n",
    "    if binary == True:\n",
    "        predicted_y = loaded_model.predict_classes(data)\n",
    "        print(predicted_y)\n",
    "    else:\n",
    "        predicted_y = loaded_model.predict(data)\n",
    "        print(predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ohne FS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T13:22:10.934138Z",
     "start_time": "2020-11-17T13:22:10.930821Z"
    }
   },
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T10:31:31.215030Z",
     "start_time": "2021-01-05T10:31:31.202625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ann_binary(directory, dataset, outcome, cutoff, categorical_features, \n",
    "                       smote_upsample, grid, model_type, split_state, save_model, batch_size,earlystopping_patience):\n",
    "    \"\"\"\n",
    "    Ann with binary outcome \n",
    "    with the availabily of upsample via SMOTE-NC\n",
    "    \"\"\"\n",
    "    \n",
    "    continous_to_binary_liu = {outcome: cutoff} \n",
    "    X, y = prepare_dataset(\"liu\", dataset, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "\n",
    "    #split in train test data 80/20 split stratified and shuffled \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state = split_state, test_size = 0.2, stratify = y, shuffle = True)\n",
    "    \n",
    "    if smote_upsample == True:\n",
    "        logging.info(\"Upsample less represented class.\")\n",
    "        #For SMOTE-NC we need to pinpoint the column position where is the categorical features are.\n",
    "        categorical_f = []\n",
    "        categorical_features.remove(outcome)\n",
    "        for feature in categorical_features:\n",
    "            categorical_f.append(X_train.columns.get_loc(feature))\n",
    "        #For SMOTE-NC we need to pinpoint the column position where is the categorical features are.\n",
    "        smotenc = SMOTENC(categorical_features=categorical_f,random_state = 101)\n",
    "        X_train, y_train = smotenc.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #create for each split a new directory \n",
    "    new_dir = directory + \"/\" + \"split_random_seed_\"+ str(split_state)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    \n",
    "    log = open(new_dir+\"/\"+\"myprog.log\", \"a+\")\n",
    "    sys.stdout = log\n",
    "\n",
    "    if model_type == \"binary\":\n",
    "        grid_search_and_val(new_dir, X_train, y_train, X_test, y_test, 'binary', grid, save_model, batch_size, \n",
    "                            earlystopping_patience, n_features=0, epochs=1000)\n",
    "    elif model_type == \"autoencoder\":\n",
    "        grid_search_and_val(new_dir, X_train, y_train, X_test, y_test, 'autoencoder', grid, save_model,batch_size, \n",
    "                            earlystopping_patience,n_features=0,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:59:49.511190Z",
     "start_time": "2020-12-08T12:59:49.502050Z"
    }
   },
   "outputs": [],
   "source": [
    "def ann_conti(directory, dataset, outcome, cutoff, categorical_features, \n",
    "                       smote_upsample, grid, split_state, save_model,batch_size, earlystopping_patience):\n",
    "    \"\"\"\n",
    "    Ann with continuously outcome \n",
    "    with the availabily of upsample via SMOTE-NC\n",
    "    \"\"\"\n",
    "    \n",
    "    continous_to_binary_liu = {}\n",
    "\n",
    "    X, y = prepare_dataset(\"liu\", dataset, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=split_state, test_size=0.2, shuffle=True)\n",
    "    \n",
    "    #create for each split a new directory \n",
    "    new_dir = directory + \"/\" + \"split_random_seed_\"+ str(split_state)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "        \n",
    "    log = open(new_dir+\"/\"+\"myprog.log\", \"a+\")\n",
    "    sys.stdout = log\n",
    "\n",
    "    grid_search_and_val(new_dir, X_train, y_train, X_test, y_test, 'continuously',\n",
    "                        grid, save_model, batch_size, earlystopping_patience ,n_features=0, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T13:25:58.047840Z",
     "start_time": "2020-11-17T13:25:58.044292Z"
    }
   },
   "source": [
    "# Mit FS (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-25T09:35:44.554177Z",
     "start_time": "2020-12-25T09:35:44.531932Z"
    }
   },
   "outputs": [],
   "source": [
    "def ann_fs_binary(directory, dataset, outcome, cutoff, categorical_features, clinical_features,\n",
    "                       smote_upsample, grid, split_state,save_model,n_features, batch_size, earlystopping_patience):\n",
    "    \"\"\"\n",
    "    Ann with feature selection (SVC) with binary outcome \n",
    "    with the availabily of upsample via SMOTE-NC\n",
    "    \"\"\"\n",
    "\n",
    "    continous_to_binary_liu = {outcome: cutoff}\n",
    "    data_for_svm =dataset.copy()\n",
    "\n",
    "    X, y = prepare_dataset(\"liu\", data_for_svm, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "\n",
    "    X_genes = X.drop(columns=clinical_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_genes, y, random_state=split_state, test_size=0.2, stratify=y)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # mit SVC\n",
    "    param_grid_SVC = dict(C=[0.1,0.2,0.4,0.4,0.6,0.7,0.8,0.9,0.99],\n",
    "                      kernel= ['linear'])\n",
    "    svm_features, selector, X_train_selected, X_test_selected = feature_selection_svc(\n",
    "        param_grid_SVC, True, X, X_train, y_train, X_test, y_test, n_features, verbose=0)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    X, y = prepare_dataset(\"liu\", dataset, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "    X_svm = X[svm_features]\n",
    "    X_new = pd.concat([X_svm, X[clinical_features]], axis=1)\n",
    "    logging.info(svm_features)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=split_state, test_size=0.2, shuffle=True, stratify = y)\n",
    "\n",
    "    if smote_upsample == True:\n",
    "        logging.info(\"Upsample less represented class.\")\n",
    "        #For SMOTE-NC we need to pinpoint the column position where is the categorical features are.\n",
    "        categorical_f = []\n",
    "        categorical_features.remove(outcome)\n",
    "        for feature in categorical_features:\n",
    "            categorical_f.append(X_new.columns.get_loc(feature))\n",
    "        smotenc = SMOTENC(categorical_features=categorical_f,random_state = 101)\n",
    "        X_train, y_train = smotenc.fit_resample(X_train, y_train)\n",
    "    \n",
    "    new_dir = directory + \"/\" + \"split_random_seed_\"+ str(split_state)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "        \n",
    "    f = open(new_dir+\"/\"+\"features.txt\", \"w+\")\n",
    "    for i in svm_features:\n",
    "        f.write(str(i)+\"\\n\")\n",
    "    f.close()\n",
    "        \n",
    "    log = open(new_dir+\"/\"+\"myprog.log\", \"a+\")\n",
    "    sys.stdout = log\n",
    "        \n",
    "    grid_search_and_val(new_dir, X_train, y_train, X_test, y_test, 'binary',\n",
    "                            grid,save_model, batch_size, earlystopping_patience, n_features, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T13:26:12.692242Z",
     "start_time": "2020-11-17T13:26:12.689048Z"
    }
   },
   "source": [
    "## Continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:43:53.405841Z",
     "start_time": "2020-12-03T12:43:53.392487Z"
    }
   },
   "outputs": [],
   "source": [
    "def ann_fs_conti(directory, dataset, outcome, cutoff, categorical_features, clinical_features,\n",
    "                       smote_upsample, grid, split_state, save_model, n_features, batch_size, earlystopping_patience):\n",
    "    \"\"\"\n",
    "    Ann with feature selection (SVR) with continuously outcome \n",
    "    \"\"\"\n",
    "    \n",
    "    continous_to_binary_liu = {}\n",
    "    data_for_svm =dataset.copy()\n",
    "\n",
    "    X, y = prepare_dataset(\"liu\", data_for_svm, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "\n",
    "    X_genes = X.drop(columns=clinical_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X_genes, y, random_state=split_state, test_size=0.2)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # mit SVC\n",
    "    param_grid_SVC = dict(C=[0.1,0.2,0.4,0.4,0.6,0.7,0.8,0.9,0.99],\n",
    "                      kernel= ['linear'])\n",
    "    svm_features, selector, X_train_selected, X_test_selected = feature_selection_svc(\n",
    "        param_grid_SVC, False, X, X_train, y_train, X_test, y_test, n_features, verbose=0)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    X, y = prepare_dataset(\"liu\", dataset, categorical_features,\n",
    "                           \"\", continous_to_binary_liu, outcome)\n",
    "\n",
    "    X_svm = X[svm_features]\n",
    "    X_new = pd.concat([X_svm, X[clinical_features]], axis=1)\n",
    "    logging.info(svm_features)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=split_state, test_size=0.2, shuffle=True)\n",
    "\n",
    "    new_dir = directory + \"/\" + \"split_random_seed_\"+ str(split_state)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "        \n",
    "    log = open(new_dir+\"/\"+\"myprog.log\", \"a+\")\n",
    "    sys.stdout = log\n",
    "        \n",
    "    grid_search_and_val(new_dir, X_train,y_train, X_test, y_test,'continuously', grid, save_model, \n",
    "                        batch_size, earlystopping_patience, n_features, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T15:17:45.678925Z",
     "start_time": "2021-01-27T15:17:45.627856Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-991535767f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mStoreDictKeyPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mClass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mvia\u001b[0m \u001b[0mArgumentParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "class StoreDictKeyPair(argparse.Action):\n",
    "    \"\"\"\n",
    "    Class to read in dictionary via ArgumentParser\n",
    "    argparse.Action \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n",
    "        self._nargs = nargs\n",
    "        super(StoreDictKeyPair, self).__init__(\n",
    "            option_strings, dest, nargs=nargs, **kwargs)\n",
    "\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        my_dict = {}\n",
    "        for kv in values:\n",
    "            k, v = kv.split(\"=\")\n",
    "            my_dict[k] = v\n",
    "        setattr(namespace, self.dest, my_dict)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define args\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='neural network based prediction')\n",
    "\n",
    "    subparser = parser.add_subparsers(dest='command')\n",
    "    ann = subparser.add_parser('ann')\n",
    "    prediction = subparser.add_parser('predict')\n",
    "\n",
    "    # predict group\n",
    "    prediction.add_argument('-j', '--json', type=str,\n",
    "                            required=True, help='path to json file')\n",
    "    prediction.add_argument('-w', '--weights', type=str,\n",
    "                            required=True, help='path to h5 file (weights)')\n",
    "    prediction.add_argument('-b', '--binary', action='store_true',\n",
    "                            help='define if continuously outcome(default) or binary')\n",
    "    prediction.add_argument('-t', '--table', type=argparse.FileType('r', encoding='UTF-8'),\n",
    "                            required=True, help='table for predictions')\n",
    "\n",
    "    # ann group\n",
    "    ann.add_argument('-a', '--ann', type=str, required=True,\n",
    "                     choices=[\"ann\", \"fs_ann\", \"autoencoder\"], help='define which NN should be run')\n",
    "    ann.add_argument('-i', '--input_data', type=argparse.FileType('r', encoding='UTF-8'),\n",
    "                     required=True, help='path to input data file')\n",
    "    ann.add_argument('-d','--output_dir', type=str, help='name of output directory', default=\"output\")\n",
    "    ann.add_argument('-o', '--outcome',  required=True,\n",
    "                     type=str, help='define outcome variable')\n",
    "    ann.add_argument('-b', '--binary', action='store_true',\n",
    "                     help='define if continuously outcome (default) or binary (if binary --cutoff required)')\n",
    "    ann.add_argument('-c', '--cutoff', type=int,\n",
    "                     help='define cutoff for classification')\n",
    "    ann.add_argument('--categorical_features', nargs='*', type=str,\n",
    "                     help='list of categorical features')\n",
    "    ann.add_argument('--clinical_features', nargs='*', type=str,\n",
    "                     help='list of clinical features')\n",
    "    ann.add_argument('-u', '--smote_upsample', action='store_true',\n",
    "                     help='optional if to use SMOTE-NC (default False)')\n",
    "    ann.add_argument(\"-g\", \"--grid\", dest=\"grid\", required=True, action=StoreDictKeyPair, nargs=\"+\",\n",
    "                     help='gridsearch parameters')\n",
    "    ann.add_argument('-s', '--split_state', type=int, default=0,\n",
    "                     help='different 80/20 split state')\n",
    "    ann.add_argument('-m', '--save_model', action='store_true',\n",
    "                     help='save best model')\n",
    "    ann.add_argument('--n_features',type=int,default=50, \n",
    "                     help=\"n_features to choose for Feature Selection\")\n",
    "    ann.add_argument('--batch_size', type=int, default=32,\n",
    "                    help=\"batch size for ANN\")\n",
    "    ann.add_argument('--early_stopping', type=int, default=20,\n",
    "                    help=\"early stopping patience for ANN\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.command == 'predict':\n",
    "        load_model_and_predict(\n",
    "            args.json, args.weights, args.binary, args.table)\n",
    "    elif args.command == 'ann':\n",
    "        # Create new directory\n",
    "        dateTimeObj = datetime.now()\n",
    "        timestampStr = dateTimeObj.strftime(\"%d_%m_%y_%H_%M\")\n",
    "        time.sleep(1)\n",
    "        if not os.path.exists(\"../results\"):\n",
    "            os.makedirs(\"../results\")\n",
    "        if not os.path.exists(\"../results/\"+args.output_dir):\n",
    "            os.makedirs(\"../results/\"+args.output_dir)\n",
    "        if args.binary == True:\n",
    "            directory = \"../results/\"+args.output_dir+\"/\"+timestampStr+\"_\"+args.ann+\"_binary\"\n",
    "        else:\n",
    "            directory = \"../results/\"+args.output_dir+\"/\"+timestampStr+\"_\"+args.ann+\"_continuously\"\n",
    "        time.sleep(2)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        # Create Logger\n",
    "        logging.basicConfig(level=logging.DEBUG, filename=directory+\"/logfile\", filemode=\"a+\",\n",
    "                            format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "        logging.info('Started')\n",
    "        logging.info(\"Input File: {}\".format(args.input_data))\n",
    "\n",
    "        # if binary then cutoff required\n",
    "        if args.binary and (args.cutoff is None):\n",
    "            parser.error(\"--binary requires --cutoff\")\n",
    "\n",
    "        if args.categorical_features == None:\n",
    "            categorical_features = []\n",
    "        else:\n",
    "            categorical_features = args.categorical_features\n",
    "        if args.clinical_features == None:\n",
    "            clinical_features = []\n",
    "        else:\n",
    "            clinical_features = args.clinical_features\n",
    "\n",
    "        # read in params for param grid and cast into ints/floats\n",
    "        dataset = load_data(args.input_data)\n",
    "        for key, value in args.grid.items():\n",
    "            values = value.split(\",\")\n",
    "            numbers = \"\"\n",
    "            if key == \"__ann_n_neurons\":\n",
    "                ann_neurons = [int(i) for i in values]\n",
    "            elif key == \"__ann_num_hidden\":\n",
    "                ann_num_hidden = [int(i) for i in values]\n",
    "            elif key == \"__ann_l1_reg\":\n",
    "                ann_l1_reg = [float(i) for i in values]\n",
    "            elif key == \"__ann_l2_reg\":\n",
    "                ann_l2_reg = [float(i) for i in values]\n",
    "            elif key == \"__ann_learning_rate\":\n",
    "                ann_learning_rate = [float(i) for i in values]\n",
    "            elif key == \"__ann_used_optimizer\":\n",
    "                ann_used_optimizer = [str(i) for i in values]\n",
    "            elif key == \"__ann_dropout_rate\":\n",
    "                ann_dropout_rate = [float(i) for i in values]\n",
    "\n",
    "        # create param_grid for GridSearchCV\n",
    "        param_grid = dict(ann__n_neurons=ann_neurons, ann__num_hidden=ann_num_hidden, ann__used_optimizer=ann_used_optimizer, \n",
    "                          ann__l1_reg=ann_l1_reg, ann__l2_reg=ann_l2_reg, ann__learning_rate=ann_learning_rate,\n",
    "                          ann__dropout_rate=ann_dropout_rate)\n",
    "\n",
    "        #Warnings tensorflow and matplotlib silent\n",
    "        tf.get_logger().setLevel(logging.ERROR)\n",
    "        mpl_logger = logging.getLogger('matplotlib')\n",
    "        mpl_logger.setLevel(logging.WARNING)\n",
    "        \n",
    "        # ann with or without feature selection\n",
    "        if args.ann == \"ann\":  # without feature selection\n",
    "            logging.info(\"-----Runs neural network-----\")\n",
    "            print(\"-----Runs neural network-----\")\n",
    "            if args.binary == True:  # binary\n",
    "                logging.info('-----Classification-----')\n",
    "                print(\"-----Classification-----\")\n",
    "                logging.info(\"Outcome: {}\".format(args.outcome))\n",
    "                logging.info(\"Cutoff: {}\".format(args.cutoff))\n",
    "                logging.info(\"Categorical Features: {}\".format(\n",
    "                    args.categorical_features))\n",
    "                logging.info(\"Clinical Features: {}\".format(\n",
    "                    args.clinical_features))\n",
    "                logging.info(\"Smote: {}\".format(args.smote_upsample))\n",
    "                logging.info(\"Chosen Grid: {}\".format(args.grid))\n",
    "                logging.info(\"80/20 Split State: {}\".format(args.split_state))\n",
    "                logging.info(\"Early stopping patience: {}\".format(args.early_stopping))\n",
    "                logging.info(\"Batch size ANN: {}\".format(args.batch_size))\n",
    "                ann_binary(directory, dataset, args.outcome, args.cutoff, categorical_features,\n",
    "                           args.smote_upsample, param_grid, \"binary\", args.split_state, args.save_model,\n",
    "                           args.batch_size,args.early_stopping)\n",
    "            else:  # continuously\n",
    "                print(\"-----Regression-----\")\n",
    "                logging.info(\"-----Regression-----\")\n",
    "                logging.info(\"Outcome: {}\".format(args.outcome))\n",
    "                logging.info(\"Categorical Features: {}\".format(\n",
    "                    args.categorical_features))\n",
    "                logging.info(\"Clinical Features: {}\".format(\n",
    "                    args.clinical_features))\n",
    "                logging.info(\"Smote: {}\".format(args.smote_upsample))\n",
    "                logging.info(\"Chosen Grid: {}\".format(args.grid))\n",
    "                logging.info(\"80/20 Split State: {}\".format(args.split_state))\n",
    "                logging.info(\"Early stopping patience: {}\".format(args.early_stopping))\n",
    "                logging.info(\"Batch size ANN: {}\".format(args.batch_size))\n",
    "                ann_conti(directory, dataset, args.outcome, args.cutoff, categorical_features,\n",
    "                          args.smote_upsample, param_grid, args.split_state, args.save_model,\n",
    "                         args.batch_size, args.early_stopping)\n",
    "\n",
    "        elif args.ann == \"fs_ann\":  # with feature selection\n",
    "            print(\"-----Runs neural network with feature selection-----\")\n",
    "            logging.info(\"Runs neural network with feature selection\")\n",
    "            if args.binary == True:  # binary\n",
    "                print(\"-----Classification-----\")\n",
    "                logging.info(\"-----Classification-----\")\n",
    "                logging.info(\"Outcome: {}\".format(args.outcome))\n",
    "                logging.info(\"Cutoff: {}\".format(args.cutoff))\n",
    "                logging.info(\"Categorical Features: {}\".format(\n",
    "                    args.categorical_features))\n",
    "                logging.info(\"Clinical Features: {}\".format(\n",
    "                    args.clinical_features))\n",
    "                logging.info(\"Smote: {}\".format(args.smote_upsample))\n",
    "                logging.info(\"Chosen Grid: {}\".format(args.grid))\n",
    "                logging.info(\"80/20 Split State: {}\".format(args.split_state))\n",
    "                logging.info(\"N_features chosen: {}\".format(args.n_features))\n",
    "                logging.info(\"Early stopping patience: {}\".format(args.early_stopping))\n",
    "                logging.info(\"Batch size ANN: {}\".format(args.batch_size))\n",
    "                ann_fs_binary(directory, dataset, args.outcome, args.cutoff, categorical_features,\n",
    "                              clinical_features, args.smote_upsample, param_grid, args.split_state, args.save_model,\n",
    "                              args.n_features,args.batch_size, args.early_stopping)\n",
    "\n",
    "            else:  # continuously\n",
    "                print(\"-----Regression-----\")\n",
    "                logging.info(\"-----Regression-----\")\n",
    "                logging.info(\"Outcome: {}\".format(args.outcome))\n",
    "                logging.info(\"Categorical Features: {}\".format(\n",
    "                    args.categorical_features))\n",
    "                logging.info(\"Clinical Features: {}\".format(\n",
    "                    args.clinical_features))\n",
    "                logging.info(\"Smote: {}\".format(args.smote_upsample))\n",
    "                logging.info(\"Chosen Grid: {}\".format(args.grid))\n",
    "                logging.info(\"80/20 Split State: {}\".format(args.split_state))\n",
    "                logging.info(\"N_features chosen: {}\".format(args.n_features))\n",
    "                logging.info(\"Early stopping patience: {}\".format(args.early_stopping))\n",
    "                logging.info(\"Batch size ANN: {}\".format(args.batch_size))\n",
    "                ann_fs_conti(directory, dataset, args.outcome, args.cutoff, categorical_features,\n",
    "                             clinical_features, args.smote_upsample, param_grid, args.split_state, args.save_model,\n",
    "                             args.n_features, args.batch_size, args.early_stopping)\n",
    "\n",
    "        elif args.ann == \"autoencoder\":\n",
    "            print(\"-----Runs neural network with autoencoder-----\")\n",
    "            logging.info(\"-----Runs neural network with autoencoder-----\")\n",
    "            if args.binary == True:  # binary\n",
    "                print(\"-----Classification-----\")\n",
    "                logging.info(\"Outcome: {}\".format(args.outcome))\n",
    "                logging.info(\"Cutoff: {}\".format(args.cutoff))\n",
    "                logging.info(\"Categorical Features: {}\".format(\n",
    "                    args.categorical_features))\n",
    "                logging.info(\"Clinical Features: {}\".format(\n",
    "                    args.clinical_features))\n",
    "                logging.info(\"Smote: {}\".format(args.smote_upsample))\n",
    "                logging.info(\"Chosen Grid: {}\".format(args.grid))\n",
    "                logging.info(\"80/20 Split State: {}\".format(args.split_state))\n",
    "                logging.info(\"Early stopping patience: {}\".format(args.early_stopping))\n",
    "                logging.info(\"Batch size ANN: {}\".format(args.batch_size))\n",
    "                ann_binary(directory, dataset, args.outcome, args.cutoff, categorical_features,\n",
    "                           args.smote_upsample, param_grid, \"autoencoder\", args.split_state, args.save_model,\n",
    "                          args.batch_size, args.early_stopping)\n",
    "\n",
    "        logging.info('Finished')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
